{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCbEA5dtAqsI"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLNZwaSdAqsK"
      },
      "source": [
        "## Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FuAJv15ksuz",
        "outputId": "ffd14c32-c5fc-463e-fa7f-a5dbadb73a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-28 14:10:09--  https://raw.githubusercontent.com/Skyhao6/IST664_Final_Proj/main/modern.nltktok\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1040850 (1016K) [text/plain]\n",
            "Saving to: ‘modern.nltktok’\n",
            "\n",
            "modern.nltktok      100%[===================>]   1016K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-02-28 14:10:10 (15.8 MB/s) - ‘modern.nltktok’ saved [1040850/1040850]\n",
            "\n",
            "--2024-02-28 14:10:10--  https://raw.githubusercontent.com/Skyhao6/IST664_Final_Proj/main/original.nltktok\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1109065 (1.1M) [text/plain]\n",
            "Saving to: ‘original.nltktok’\n",
            "\n",
            "original.nltktok    100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-02-28 14:10:10 (15.5 MB/s) - ‘original.nltktok’ saved [1109065/1109065]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Skyhao6/IST664_Final_Proj/main/modern.nltktok\n",
        "!wget https://raw.githubusercontent.com/Skyhao6/IST664_Final_Proj/main/original.nltktok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRygP8TVAqsK",
        "outputId": "f102f3f9-0d40-4b63-f455-c0936b25f09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of source sentences:  21076\n",
            "number of target sentences:  21076\n",
            "1st three sentences in the source and target: \n",
            "[\"I have a mind to strike thee ere thou speak'st .\", \"Yet if thou say Antony lives , is well , Or friends with Caesar , or not captive to him , I'll set thee in a shower of gold and hail Rich pearls upon thee .\", \"Madam , he's well .\"]\n",
            "['[start] I have half a mind to hit you before you speak again . [end]', \"[start] But if Antony is alive , healthy , friendly with Caesar , and not Caesar's prisoner , I'll shower you with gold and pearls . [end]\", \"[start] Madam , he's well . [end]\"]\n"
          ]
        }
      ],
      "source": [
        "# Read in the data\n",
        "# https://github.com/harsh19/Shakespearizing-Modern-English/tree/master/data\n",
        "# source: modern.nltktok\n",
        "# target: original.nltktok\n",
        "# Task: Translate middle English to modern English\n",
        "source = []\n",
        "target = []\n",
        "\n",
        "with open('original.nltktok', 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "    source = content.split('\\n')\n",
        "\n",
        "with open('modern.nltktok', 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "    target = content.split('\\n')\n",
        "\n",
        "# Add start and end tokens to target\n",
        "for i in range(len(target)):\n",
        "    target[i] = '[start] ' + target[i] + ' [end]'\n",
        "\n",
        "# Examples\n",
        "print('number of source sentences: ', len(source))\n",
        "print('number of target sentences: ', len(target))\n",
        "print('1st three sentences in the source and target: ')\n",
        "print(source[:3])\n",
        "print(target[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWF5XS0yAqsL",
        "outputId": "446aea1e-e172-4e91-fcd2-55b177810917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min length of source:  0\n",
            "max length of source:  19\n",
            "min length of target:  2\n",
            "max length of target:  18\n"
          ]
        }
      ],
      "source": [
        "# Statistical information\n",
        "min_len = 100000\n",
        "max_len = 0\n",
        "for sentence in source:\n",
        "    if len(sentence) < min_len:\n",
        "        min_len = len(sentence.split())\n",
        "    if len(sentence) > max_len:\n",
        "        max_len = len(sentence.split())\n",
        "\n",
        "print(\"min length of source: \", min_len)\n",
        "print(\"max length of source: \", max_len)\n",
        "\n",
        "min_len = 100000\n",
        "max_len = 0\n",
        "for sentence in target:\n",
        "    if len(sentence) < min_len:\n",
        "        min_len = len(sentence.split())\n",
        "    if len(sentence) > max_len:\n",
        "        max_len = len(sentence.split())\n",
        "\n",
        "print(\"min length of target: \", min_len)\n",
        "print(\"max length of target: \", max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwvd5L7WAqsL"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NhnDy_9AqsM"
      },
      "outputs": [],
      "source": [
        "from keras.layers import TextVectorization\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "# Vocabulary size of source and target\n",
        "INPUT_MAX_TOKENS = 13000\n",
        "OUTPUT_MAX_TOKENS = 10000\n",
        "# Max length of source and target sentence\n",
        "# If the sentence is shorter than MAX_LEN, it will be padded with 0\n",
        "# If the sentence is longer than MAX_LEN, it will be truncated\n",
        "MAX_LEN = 30\n",
        "batch_size = 64\n",
        "\n",
        "# Build source tokenizer\n",
        "source_tokenizer = TextVectorization(\n",
        "    max_tokens=INPUT_MAX_TOKENS,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_LEN)\n",
        "\n",
        "source_tokenizer.adapt(source)\n",
        "\n",
        "\n",
        "# Build target tokenizer\n",
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "target_tokenizer = TextVectorization(\n",
        "    max_tokens=OUTPUT_MAX_TOKENS,\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        "    standardize=custom_standardization,\n",
        "    )\n",
        "target_tokenizer.adapt(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acg27rTWAqsM",
        "outputId": "1f962c1c-5ca9-4fee-f734-1d02fb0f4978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source_vocab_size:  12443\n",
            "target_vocab_size:  9977\n"
          ]
        }
      ],
      "source": [
        "source_vocab_size = len(source_tokenizer.get_vocabulary())\n",
        "target_vocab_size = len(target_tokenizer.get_vocabulary())\n",
        "print(\"source_vocab_size: \", source_vocab_size)\n",
        "print(\"target_vocab_size: \", target_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfyXFRe6AqsM",
        "outputId": "9bec47c5-d112-4af2-996d-2b00fbaf31da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[start] what oclock tomorrow shall i send to thee  [end]\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "a = \"[start] What o'clock tomorrow Shall I send to thee ? [end]\"\n",
        "print(custom_standardization(a).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8VWfMDYAqsN",
        "outputId": "3b063ab2-25ae-4c1c-e316-03d59662dd1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Let him be the devil , an he will , I care not .', 'Niggard of question , but of our demands Most free in his reply .', 'Wherefore ?']\n",
            "[\"[start] Let him be the devil if he wants to , I don't care . [end]\", \"[start] He didn't ask questions , but answered ours at length . [end]\", '[start] A reason ? [end]']\n"
          ]
        }
      ],
      "source": [
        "# Randomly shuffle data\n",
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "# Combine the lists into pairs\n",
        "combined_lists = list(zip(source, target))\n",
        "\n",
        "# Shuffle the combined list\n",
        "random.shuffle(combined_lists)\n",
        "\n",
        "# Unzip the shuffled list back into separate lists\n",
        "source, target = zip(*combined_lists)\n",
        "\n",
        "source = list(source)\n",
        "target = list(target)\n",
        "print(source[:3])\n",
        "print(target[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c415SSYdAqsN",
        "outputId": "ca30ee9a-7074-4f7b-9700-76cf2bde2919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of source sentences:  21076\n",
            "Total number of target sentences:  21076\n"
          ]
        }
      ],
      "source": [
        "# Split data into training and test set\n",
        "print(\"Total number of source sentences: \", len(source))\n",
        "print(\"Total number of target sentences: \", len(source))\n",
        "\n",
        "source_train = source[:int(len(source)*0.9)]\n",
        "target_train = target[:int(len(target)*0.9)]\n",
        "\n",
        "source_test = source[int(len(source)*0.9):]\n",
        "target_test = target[int(len(target)*0.9):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjRc0YjyAqsN"
      },
      "outputs": [],
      "source": [
        "src_word_to_idx = dict([(v,k) for k, v in enumerate(source_tokenizer.get_vocabulary())])\n",
        "src_idx_to_word = dict([(k,v) for k, v in enumerate(source_tokenizer.get_vocabulary())])\n",
        "tar_word_to_idx = dict([(v,k) for k, v in enumerate(target_tokenizer.get_vocabulary())])\n",
        "tar_idx_to_word = dict([(k,v) for k, v in enumerate(target_tokenizer.get_vocabulary())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm8AaL10AqsN"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = source_tokenizer(source_train)\n",
        "decoder_input_data = target_tokenizer(target_train)[:, :-1]\n",
        "decoder_output_data = target_tokenizer(target_train)[:, 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wFhzY02AqsN",
        "outputId": "c6b3d0bc-75e8-4b85-fed6-3cb0b782e7e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(18968, 30)\n",
            "(18968, 30)\n",
            "(18968, 30)\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)\n",
        "print(decoder_output_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCf24XZJAqsN",
        "outputId": "767567d6-22d8-48ba-cdb9-e11fa50a9acf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source:  For us , you know Whose he is we are , and that is Caesar's .\n",
            "Target:  [start] As for us , you know we are Antony's , and he is Caesar's . [end]\n",
            "Decoder input:  tf.Tensor(\n",
            "[   2   24   18   91    4   69   47   28 1318    8   22   14  482    3\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(30,), dtype=int64)\n",
            "Decoder output:  tf.Tensor(\n",
            "[  24   18   91    4   69   47   28 1318    8   22   14  482    3    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(30,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Examples\n",
        "# Encoder input: i have half a mind to hit you before you speak again\n",
        "# Decoder input: [start] i have a mind to strike thee ere thou speakst\n",
        "# Decoder output: i have a mind to strike thee ere thou speakst [end]\n",
        "print(\"Source: \", source_train[664])\n",
        "print(\"Target: \", target_train[664])\n",
        "\n",
        "\n",
        "\n",
        "print(\"Decoder input: \", decoder_input_data[664])\n",
        "print(\"Decoder output: \", decoder_output_data[664])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcPXwrEEAqsN"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRRYAiSlAqsO"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "# d_model\n",
        "embedding_size = 512\n",
        "\n",
        "# dff = d_model * 4\n",
        "dense_dim = 2048\n",
        "\n",
        "# num_heads\n",
        "n_head = 8\n",
        "\n",
        "# num_layers\n",
        "n_layer = 1\n",
        "\n",
        "# dropout rate\n",
        "dropout = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQIVtMLeAqsO"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0USvvGsTAqsO"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import Model\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Positional encoding\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(pos, d_model):\n",
        "\n",
        "    def get_angles(position, i):\n",
        "        return position / np.power(10000., 2. * (i // 2.) / np.float32(d_model))\n",
        "\n",
        "    angle_rates = get_angles(np.arange(pos)[:, np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis, :])\n",
        "    pe_sin = np.sin(angle_rates[:, 0::2])\n",
        "    pe_cos = np.cos(angle_rates[:, 1::2])\n",
        "    pos_encoding = np.concatenate([pe_sin, pe_cos], axis=-1)\n",
        "    pos_encoding = tf.cast(pos_encoding[np.newaxis, ...], tf.float32)\n",
        "    return pos_encoding\n",
        "\n",
        "# Masking\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, np.newaxis, np.newaxis, :]\n",
        "\n",
        "\n",
        "# Look-ahead mask for decoder\n",
        "def create_look_ahead_mask(size):\n",
        "\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # shape=[seq_len, seq_len]\n",
        "\n",
        "def create_mask(inputs, targets):\n",
        "    encoder_padding_mask = create_padding_mask(inputs)\n",
        "    decoder_padding_mask = create_padding_mask(inputs)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
        "    decoder_targets_padding_mask = create_padding_mask(targets)\n",
        "    combined_mask = tf.maximum(decoder_targets_padding_mask, look_ahead_mask)\n",
        "    return encoder_padding_mask, combined_mask, decoder_padding_mask\n",
        "\n",
        "# Split tensor into (batch_size, n_head, seq_len, d_head)\n",
        "def splite_tensor(tensor):\n",
        "    shape = tf.shape(tensor)\n",
        "    tensor = tf.reshape(\n",
        "        tensor, shape=[shape[0], -1, n_head, embedding_size//n_head])\n",
        "    tensor = tf.transpose(tensor, perm=[0, 2, 1, 3])\n",
        "    return tensor\n",
        "\n",
        "class MultiHeadAttentionLayer(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "\n",
        "    # Define the layers needed for the computation, including the input shape\n",
        "    def build(self, input_shape):\n",
        "        # Input shape: [batch_size, seq_len, embedding_size] for query, key, value\n",
        "        self.dense_query = layers.Dense(\n",
        "            units=embedding_size, activation='relu')\n",
        "        self.dense_key = layers.Dense(units=embedding_size, activation='relu')\n",
        "        self.dense_value = layers.Dense(\n",
        "            units=embedding_size, activation='relu')\n",
        "\n",
        "        self.layer_norm = layers.LayerNormalization()\n",
        "        super(MultiHeadAttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs\n",
        "        shape = tf.shape(query)\n",
        "\n",
        "        query_dense = self.dense_query(query)\n",
        "        key_dense = self.dense_key(key)\n",
        "        value_dense = self.dense_value(value)\n",
        "\n",
        "        query_dense = splite_tensor(query_dense)\n",
        "        key_dense = splite_tensor(key_dense)\n",
        "        value_dense = splite_tensor(value_dense)\n",
        "\n",
        "        attention = tf.matmul(query_dense, key_dense, transpose_b=True) / \\\n",
        "            tf.math.sqrt(tf.cast(embedding_size, tf.float32))\n",
        "        attention += (mask*-1e9)\n",
        "        attention = tf.nn.softmax(attention)\n",
        "        attention = layers.Dropout(0.1)(attention)\n",
        "        attention = tf.matmul(attention, value_dense)\n",
        "        attention = tf.transpose(attention, [0, 2, 1, 3])\n",
        "        attention = tf.reshape(attention, [shape[0], -1, embedding_size])\n",
        "\n",
        "        attention = self.layer_norm((attention+query))\n",
        "        return attention\n",
        "\n",
        "\n",
        "# Base encoder layer\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, n_head, emb_dim, dense_dim, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attn = MultiHeadAttentionLayer()\n",
        "        self.drop_attn = layers.Dropout(dropout)\n",
        "        self.dense1 = layers.Dense(dense_dim, activation='relu')\n",
        "        self.dense2 = layers.Dense(emb_dim)\n",
        "        self.drop_dense = layers.Dropout(dropout)\n",
        "        self.layer_norm_attn = layers.LayerNormalization()\n",
        "        self.layer_norm_dense = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs,training=None):\n",
        "\n",
        "        encoder_inputs,mask = inputs\n",
        "        att_out = self.attn([encoder_inputs, encoder_inputs,encoder_inputs,mask])\n",
        "        att_out = self.drop_attn(att_out, training=training)\n",
        "        att_out = self.layer_norm_attn(encoder_inputs+att_out)\n",
        "\n",
        "        dense = self.dense1(att_out)\n",
        "        dense = self.dense2(dense)\n",
        "        dense = self.drop_dense(dense, training=training)\n",
        "        x = self.layer_norm_dense(att_out+dense)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Base encoder using multiple encoder layers\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, vocab, emb_dim, dense_dim, n_layers, n_head, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.emb = layers.Embedding(input_dim=vocab, output_dim=emb_dim)\n",
        "        self.pos = positional_encoding(MAX_LEN, emb_dim)\n",
        "\n",
        "        # multi encoder layers\n",
        "        self.encoder_layers = [EncoderLayer(\n",
        "            emb_dim=emb_dim, n_head=n_head, dense_dim=dense_dim, dropout=dropout) for _ in range(n_layers)]\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs,training=False):\n",
        "\n",
        "        encoder_inputs,mask = inputs\n",
        "        # shape=[batch_size, seq_len, d_model]\n",
        "        seq_len = encoder_inputs.shape[1]\n",
        "        # shape=[batch_size, seq_len, d_model]\n",
        "        word_embedding = self.emb(encoder_inputs)\n",
        "        word_embedding *= tf.math.sqrt(tf.cast(self.emb_dim, tf.float32))\n",
        "        emb = word_embedding + self.pos[:, :seq_len, :]\n",
        "\n",
        "        # emb = self.emb(inputs)\n",
        "        x = self.dropout(emb, training=training)\n",
        "        # print('pos:',x)\n",
        "\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer([x,mask])\n",
        "            #print('x:',x.shape)\n",
        "        return x\n",
        "\n",
        "    # def compute_mask(self, inputs, mask=None):\n",
        "    #     return self.emb.compute_mask(inputs)\n",
        "\n",
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, n_head, emb_dim, dense_dim, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.attn1 = MultiHeadAttentionLayer()\n",
        "        self.layer_norm_attn1 = layers.LayerNormalization()\n",
        "        self.drop_attn1 = layers.Dropout(dropout)\n",
        "\n",
        "        self.attn2 = MultiHeadAttentionLayer()\n",
        "        self.layer_norm_attn2 = layers.LayerNormalization()\n",
        "        self.drop_attn2 = layers.Dropout(dropout)\n",
        "\n",
        "        self.dense1 = layers.Dense(dense_dim, activation='relu')\n",
        "        self.dense2 = layers.Dense(emb_dim)\n",
        "        self.drop_dense = layers.Dropout(dropout)\n",
        "        self.layer_norm_dense = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs,training=None):\n",
        "        # causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        # if mask is not None:\n",
        "        #     padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "        #     padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        decoder_inputs, encoder_outputs,mask1,mask2 = inputs\n",
        "        att_out1 = self.attn1([decoder_inputs,decoder_inputs,decoder_inputs, mask1])\n",
        "        att_out1 = self.drop_attn1(att_out1, training=training)\n",
        "        att_out1 = self.layer_norm_attn1(decoder_inputs+att_out1)\n",
        "\n",
        "        att_out2 = self.attn2([att_out1,encoder_outputs, encoder_outputs,mask2])\n",
        "        att_out2 = self.drop_attn2(att_out2, training=training)\n",
        "        att_out2 = self.layer_norm_attn2(att_out1+att_out2)\n",
        "\n",
        "        dense = self.dense1(att_out2)\n",
        "        dense = self.dense2(dense)\n",
        "        dense = self.drop_dense(dense, training=training)\n",
        "        x = self.layer_norm_dense(att_out2+dense)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab, n_head, n_layers, emb_dim, dense_dim, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.emb = layers.Embedding(input_dim=vocab, output_dim=emb_dim)\n",
        "        self.pos = positional_encoding(MAX_LEN, emb_dim)\n",
        "\n",
        "        self.decoder_layers = [DecoderLayer(\n",
        "            n_head=n_head, emb_dim=emb_dim, dense_dim=dense_dim, dropout=dropout) for _ in range(n_layers)]\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs,training=False):\n",
        "\n",
        "        decoder_inputs, encoder_outputs,mask1,mask2 = inputs\n",
        "\n",
        "        seq_len = decoder_inputs.shape[1]\n",
        "\n",
        "        word_embedding = self.emb(decoder_inputs)\n",
        "        word_embedding *= tf.math.sqrt(tf.cast(self.emb_dim, tf.float32))\n",
        "        emb = word_embedding + self.pos[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(emb, training=training)\n",
        "\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            x = decoder_layer([x, encoder_outputs,mask1,mask2])\n",
        "\n",
        "        return x\n",
        "\n",
        "    # def compute_mask(self, inputs, mask=None):\n",
        "    #     return self.emb.compute_mask(inputs)\n",
        "class Transformer(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Transformer,self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(vocab=INPUT_MAX_TOKENS, n_head=n_head, n_layers=n_layer,\n",
        "                      emb_dim=embedding_size, dense_dim=dense_dim, dropout=dropout)\n",
        "\n",
        "        self.decoder = Decoder(vocab=OUTPUT_MAX_TOKENS, n_head=n_head, n_layers=n_layer,\n",
        "                      emb_dim=embedding_size, dense_dim=dense_dim, dropout=dropout)\n",
        "\n",
        "        self.dense = layers.Dense(OUTPUT_MAX_TOKENS, activation='softmax')\n",
        "\n",
        "    def call(self,encoder_inputs,decoder_inputs):\n",
        "        encoder_padding_mask, look_ahead_mask, decoder_padding_mask = create_mask(\n",
        "            encoder_inputs, decoder_inputs)\n",
        "\n",
        "        encoder_outputs = self.encoder([encoder_inputs,encoder_padding_mask])\n",
        "        x = self.decoder([decoder_inputs, encoder_outputs,look_ahead_mask,decoder_padding_mask])\n",
        "        x = layers.Dropout(0.1)(x)\n",
        "        decoder_outputs = self.dense(x)\n",
        "        return decoder_outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnVModiYAqsO"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr-0LpR3AqsO",
        "outputId": "5a2a4387-cd38-41fe-8cad-cebc5e5701e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer (Transformer)      (None, 30, 10000)    23477520    ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,477,520\n",
            "Trainable params: 23,477,520\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "267/267 [==============================] - 14s 33ms/step - loss: 1.8383 - acc: 0.7282 - val_loss: 1.6295 - val_acc: 0.7493\n",
            "Epoch 2/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 1.4880 - acc: 0.7654 - val_loss: 1.5120 - val_acc: 0.7654\n",
            "Epoch 3/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 1.3290 - acc: 0.7849 - val_loss: 1.4691 - val_acc: 0.7739\n",
            "Epoch 4/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 1.2085 - acc: 0.8000 - val_loss: 1.4459 - val_acc: 0.7788\n",
            "Epoch 5/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 1.1052 - acc: 0.8136 - val_loss: 1.4750 - val_acc: 0.7787\n",
            "Epoch 6/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 1.0131 - acc: 0.8264 - val_loss: 1.4805 - val_acc: 0.7791\n",
            "Epoch 7/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.9282 - acc: 0.8385 - val_loss: 1.5112 - val_acc: 0.7788\n",
            "Epoch 8/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.8509 - acc: 0.8490 - val_loss: 1.5484 - val_acc: 0.7783\n",
            "Epoch 9/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.7817 - acc: 0.8601 - val_loss: 1.5879 - val_acc: 0.7782\n",
            "Epoch 10/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.7190 - acc: 0.8700 - val_loss: 1.6352 - val_acc: 0.7756\n",
            "Epoch 11/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.6729 - acc: 0.8784 - val_loss: 1.6745 - val_acc: 0.7745\n",
            "Epoch 12/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.6317 - acc: 0.8863 - val_loss: 1.7111 - val_acc: 0.7739\n",
            "Epoch 13/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.5983 - acc: 0.8932 - val_loss: 1.7497 - val_acc: 0.7715\n",
            "Epoch 14/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.5753 - acc: 0.8987 - val_loss: 1.7715 - val_acc: 0.7715\n",
            "Epoch 15/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.5531 - acc: 0.9042 - val_loss: 1.7723 - val_acc: 0.7726\n",
            "Epoch 16/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.5282 - acc: 0.9093 - val_loss: 1.8127 - val_acc: 0.7723\n",
            "Epoch 17/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.5083 - acc: 0.9138 - val_loss: 1.8131 - val_acc: 0.7717\n",
            "Epoch 18/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.4917 - acc: 0.9172 - val_loss: 1.8279 - val_acc: 0.7715\n",
            "Epoch 19/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.4760 - acc: 0.9204 - val_loss: 1.8517 - val_acc: 0.7708\n",
            "Epoch 20/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4652 - acc: 0.9231 - val_loss: 1.8525 - val_acc: 0.7705\n",
            "Epoch 21/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4533 - acc: 0.9255 - val_loss: 1.8783 - val_acc: 0.7698\n",
            "Epoch 22/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4435 - acc: 0.9277 - val_loss: 1.9034 - val_acc: 0.7690\n",
            "Epoch 23/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4335 - acc: 0.9296 - val_loss: 1.8987 - val_acc: 0.7672\n",
            "Epoch 24/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4253 - acc: 0.9318 - val_loss: 1.9040 - val_acc: 0.7690\n",
            "Epoch 25/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4189 - acc: 0.9330 - val_loss: 1.9278 - val_acc: 0.7675\n",
            "Epoch 26/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4115 - acc: 0.9343 - val_loss: 1.9352 - val_acc: 0.7687\n",
            "Epoch 27/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4086 - acc: 0.9352 - val_loss: 1.9614 - val_acc: 0.7679\n",
            "Epoch 28/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.4034 - acc: 0.9363 - val_loss: 1.9390 - val_acc: 0.7680\n",
            "Epoch 29/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3971 - acc: 0.9377 - val_loss: 1.9723 - val_acc: 0.7682\n",
            "Epoch 30/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3911 - acc: 0.9385 - val_loss: 1.9778 - val_acc: 0.7673\n",
            "Epoch 31/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3867 - acc: 0.9396 - val_loss: 1.9988 - val_acc: 0.7687\n",
            "Epoch 32/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3851 - acc: 0.9402 - val_loss: 1.9985 - val_acc: 0.7678\n",
            "Epoch 33/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3794 - acc: 0.9412 - val_loss: 2.0082 - val_acc: 0.7664\n",
            "Epoch 34/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3756 - acc: 0.9415 - val_loss: 2.0262 - val_acc: 0.7675\n",
            "Epoch 35/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3723 - acc: 0.9425 - val_loss: 2.0204 - val_acc: 0.7650\n",
            "Epoch 36/50\n",
            "267/267 [==============================] - 8s 29ms/step - loss: 0.3720 - acc: 0.9431 - val_loss: 2.0414 - val_acc: 0.7670\n",
            "Epoch 37/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3658 - acc: 0.9437 - val_loss: 2.0587 - val_acc: 0.7677\n",
            "Epoch 38/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3639 - acc: 0.9441 - val_loss: 2.0563 - val_acc: 0.7664\n",
            "Epoch 39/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3592 - acc: 0.9450 - val_loss: 2.0582 - val_acc: 0.7669\n",
            "Epoch 40/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3573 - acc: 0.9453 - val_loss: 2.0864 - val_acc: 0.7682\n",
            "Epoch 41/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3542 - acc: 0.9462 - val_loss: 2.0700 - val_acc: 0.7656\n",
            "Epoch 42/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3543 - acc: 0.9462 - val_loss: 2.0931 - val_acc: 0.7669\n",
            "Epoch 43/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3506 - acc: 0.9466 - val_loss: 2.1080 - val_acc: 0.7653\n",
            "Epoch 44/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3476 - acc: 0.9471 - val_loss: 2.1093 - val_acc: 0.7666\n",
            "Epoch 45/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3443 - acc: 0.9473 - val_loss: 2.1243 - val_acc: 0.7660\n",
            "Epoch 46/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3433 - acc: 0.9474 - val_loss: 2.1346 - val_acc: 0.7653\n",
            "Epoch 47/50\n",
            "267/267 [==============================] - 8s 31ms/step - loss: 0.3402 - acc: 0.9481 - val_loss: 2.1442 - val_acc: 0.7660\n",
            "Epoch 48/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3380 - acc: 0.9487 - val_loss: 2.1445 - val_acc: 0.7661\n",
            "Epoch 49/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3369 - acc: 0.9490 - val_loss: 2.1598 - val_acc: 0.7652\n",
            "Epoch 50/50\n",
            "267/267 [==============================] - 8s 30ms/step - loss: 0.3346 - acc: 0.9491 - val_loss: 2.1712 - val_acc: 0.7656\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x28ea60dded0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_inputs = layers.Input((None,))\n",
        "decoder_inputs = layers.Input((None,))\n",
        "\n",
        "transformer = Transformer()\n",
        "decoder_outputs=transformer(encoder_inputs,decoder_inputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', metrics=['acc'],\n",
        "              loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
        "          validation_split=0.1, epochs=50, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-76ikFwAqsO"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o4413zKAqsO",
        "outputId": "0f83cf3c-34cc-4302-d3bd-61d22413555c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, decoder_layer_call_fn, decoder_layer_call_and_return_conditional_losses, dense_4_layer_call_fn while saving (showing 5 of 76). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: shakespearish\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: shakespearish\\assets\n"
          ]
        }
      ],
      "source": [
        "model.save('shakespearish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLnJWxEUAqsO"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('shakespearish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubt0UzQiAqsO"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX91CObZAqsO"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_tokenizer([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(MAX_LEN):\n",
        "        tokenized_target_sentence = target_tokenizer([decoded_sentence])[:, :-1]\n",
        "        predictions = model(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = tar_idx_to_word[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mElOZkDHAqsO",
        "outputId": "6b37e224-8379-45db-95f0-af15308af408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "source sentence: Thanks , you the valiant of this warlike isle That so approve the Moor .\n",
            "target sentence: [start] Thanks , you brave men who defend this island and respect Othello . [end]\n",
            "translated sentence: [start] thanks for the brave moor [end]\n",
            "==================================================\n",
            "source sentence: What , so brief ?\n",
            "target sentence: [start] That's it ? [end]\n",
            "translated sentence: [start] so what is it [end]\n",
            "==================================================\n",
            "source sentence: So long ?\n",
            "target sentence: [start] As long as that ? [end]\n",
            "translated sentence: [start] so long that long a long time [end]\n",
            "==================================================\n",
            "source sentence: I shall attend you presently at your tent .\n",
            "target sentence: [start] I'll meet you at your tent . [end]\n",
            "translated sentence: [start] ill wait for you at your sleep [end]\n",
            "==================================================\n",
            "source sentence: Good day and happiness , dear Rosalind .\n",
            "target sentence: [start] Good day and happiness to you , darling Rosalind . [end]\n",
            "translated sentence: [start] good day and happiness will rosalind [end]\n",
            "==================================================\n",
            "source sentence: Ha ?\n",
            "target sentence: [start] What ? [end]\n",
            "translated sentence: [start] ha [end]\n",
            "==================================================\n",
            "source sentence: Treason !\n",
            "target sentence: [start] Treason ! [end]\n",
            "translated sentence: [start] treason [end]\n"
          ]
        }
      ],
      "source": [
        "sentences = source_test[-7:]\n",
        "for sentence in sentences:\n",
        "    print(\"=\"*50)\n",
        "    print(\"source sentence:\", sentence)\n",
        "    print(\"target sentence:\", target_test[source_test.index(sentence)])\n",
        "    print(\"translated sentence:\", decode_sequence(sentence))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}